{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkg53u3Xhjn2"
   },
   "source": [
    "## Overall Process of Simple Tokenization\n",
    "\n",
    "Tokenization is a process of segregating the sentence into tokens. Here is a very simple tokenizer using 'tr', but we are also sorting (sort), removing duplicate lines (uniq), and then examining just the top few lines (head).  Note that \"-c\" counts the repeated lines while removing duplicates.\n",
    "\n",
    "Note: In Ubuntu Linux shell, uppercase and mixed case show up as two different lists:\n",
    "\n",
    "```\n",
    "tr -sc 'A-Za-z' '\\n' < shakes.txt | sort | uniq -c | head\n",
    "      1\n",
    "   2405 A\n",
    "     72 AARON\n",
    "     16 ABBESS\n",
    "      3 ABBOT\n",
    "      8 ABERGAVENNY\n",
    "     18 ABHORSON\n",
    "      2 ABOUT\n",
    "      6 ABRAM\n",
    "     79 ACHILLES\n",
    "     ....\n",
    "      25 Aaron\n",
    "      1 Abandon\n",
    "      1 Abandoner\n",
    "      6 Abate\n",
    "      1 Abates\n",
    "      5 Abbess\n",
    "      6 Abbey\n",
    "      5 Abbot\n",
    "      2 Abel\n",
    "      2 Aberga\n",
    "      1 Abetting\n",
    "```\n",
    "But inside colab there there's a slight distinction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1810,
     "status": "ok",
     "timestamp": 1661219910612,
     "user": {
      "displayName": "Bonnie Dorr",
      "userId": "05562801915063225516"
     },
     "user_tz": 240
    },
    "id": "634b5m0fzJh-",
    "outputId": "494e8aeb-16a2-4f51-aa34-4afb6bed9240"
   },
   "outputs": [],
   "source": [
    "!tr -sc 'A-Za-z' '\\n' < shakes.txt | sort | uniq -c | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MshbnWK9tZWZ"
   },
   "source": [
    "## Simple segregation of the sentence into tokens (without sorting or removing duplicates).\n",
    "\n",
    "Here we separate into tokens, but without sorting or removing duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 236,
     "status": "ok",
     "timestamp": 1661219967664,
     "user": {
      "displayName": "Bonnie Dorr",
      "userId": "05562801915063225516"
     },
     "user_tz": 240
    },
    "id": "1eEV4xZhz03T",
    "outputId": "7fd9a3c9-1a95-446d-aaf3-f1b007b54a24"
   },
   "outputs": [],
   "source": [
    "!tr -sc 'A-Za-z' '\\n' < shakes.txt | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AseJuI5Kv9Mc"
   },
   "source": [
    "## Segregation of the sentence into tokens with sorting\n",
    "\n",
    "Here we are sorting, but without duplicate removal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1698,
     "status": "ok",
     "timestamp": 1661041025310,
     "user": {
      "displayName": "Bonnie Dorr",
      "userId": "05562801915063225516"
     },
     "user_tz": 240
    },
    "id": "ZGmywhS_wRUm",
    "outputId": "ab740827-1c6b-427b-b04f-ca115a9b5db7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "!tr -sc 'A-Za-z' '\\n' < shakes.txt | sort | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WArvZgGFw2Wi"
   },
   "source": [
    "## Try the same command with 'sort -u' instead of 'uniq -c'\n",
    "\n",
    "Is it the same? Almost! Same as uniq, but doesn't count up the duplicated lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1737,
     "status": "ok",
     "timestamp": 1661220072489,
     "user": {
      "displayName": "Bonnie Dorr",
      "userId": "05562801915063225516"
     },
     "user_tz": 240
    },
    "id": "H-tQ7L3Zx2T7",
    "outputId": "51249abe-e9e7-4dfe-f433-dfb438a63f16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "a\n",
      "A\n",
      "Aaron\n",
      "AARON\n",
      "abaissiez\n",
      "abandon\n",
      "Abandon\n",
      "abandoned\n",
      "Abandoner\n"
     ]
    }
   ],
   "source": [
    "!tr -sc 'A-Za-z' '\\n' < shakes.txt | sort -u | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSCibP8kyWiA"
   },
   "source": [
    "## More Counting: Merging upper and lower case\n",
    "\n",
    "Example: Here 'a' and 'A' are considered the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1428,
     "status": "ok",
     "timestamp": 1661220123569,
     "user": {
      "displayName": "Bonnie Dorr",
      "userId": "05562801915063225516"
     },
     "user_tz": 240
    },
    "id": "amQ_DxXuyeqg",
    "outputId": "bba7da35-ee50-47cb-c010-fa472e5579b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1 \n",
      "  16384 a\n",
      "     97 aaron\n",
      "      1 abaissiez\n",
      "     10 abandon\n",
      "      2 abandoned\n",
      "      1 abandoner\n",
      "      2 abase\n",
      "      1 abash\n",
      "     15 abate\n"
     ]
    }
   ],
   "source": [
    "!tr 'A-Z' 'a-z' < shakes.txt | tr -sc 'a-z' '\\n' | sort | uniq -c | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uFrsSpj0AS3"
   },
   "source": [
    "## More Counting: Do the above step, but then sort the words by their counts.\n",
    "\n",
    "Here I'm taking a few more lines using 'head' so that we can see some of the more interesting cases a bit further down on the list. (What are \"d\" and \"s\"?) Note: -n sorts with numeric data and -r reverse the order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1678,
     "status": "ok",
     "timestamp": 1661042555571,
     "user": {
      "displayName": "Bonnie Dorr",
      "userId": "05562801915063225516"
     },
     "user_tz": 240
    },
    "id": "8rT9fIgn0LSK",
    "outputId": "b01e62d2-03da-4c51-9676-737a3caa1e83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  30280 the\n",
      "  28468 and\n",
      "  23971 i\n",
      "  21268 to\n",
      "  18834 of\n",
      "  16384 a\n",
      "  14695 you\n",
      "  13199 my\n",
      "  12415 in\n",
      "  12257 that\n",
      "   9920 is\n",
      "   9086 not\n",
      "   8917 d\n",
      "   8544 with\n",
      "   8431 s\n",
      "   8305 for\n",
      "   8287 me\n",
      "   8250 it\n",
      "   7588 his\n",
      "   7408 be\n"
     ]
    }
   ],
   "source": [
    "!tr 'A-Z' 'a-z' < shakes.txt | tr -sc 'a-z' '\\n' | sort | uniq -c | sort -n -r | head -n 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvq3YAuk7gT6"
   },
   "source": [
    "## 4 Tokenization in Python using \"re\"\n",
    "\n",
    "4.1 Recall the very simple ELIZA example (no preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 230,
     "status": "ok",
     "timestamp": 1661220224654,
     "user": {
      "displayName": "Bonnie Dorr",
      "userId": "05562801915063225516"
     },
     "user_tz": 240
    },
    "id": "jQgsv2529OYp",
    "outputId": "eea8cc1a-ec6b-4d6b-cb9b-77ff7a2b51f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WHY DO YOU THINK YOU ARE SAD'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "input = 'I AM SAD'\n",
    "re.sub('I AM (DEPRESSED|SAD)', r'WHY DO YOU THINK YOU ARE \\1', input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82W3IuWL9A6w"
   },
   "source": [
    "4.2 Now let's apply a more complex regular expression that removes punctuation and splits off special characters like +, /, and -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 200,
     "status": "ok",
     "timestamp": 1661220261148,
     "user": {
      "displayName": "Bonnie Dorr",
      "userId": "05562801915063225516"
     },
     "user_tz": 240
    },
    "id": "xpXiwm-g7fam",
    "outputId": "76e9fa09-5a51-4b3d-f027-0f71e6a4921b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'cat', 'weighs', '5', 'kg', '+', '/', '-', '2', 'grams']\n",
      "['I', 'live', 'in', 'the', 'U.S.A.']\n",
      "[\"I'm\", '100%', 'sure', 'it', 'costs', '40.00']\n",
      "[\"What's\", 'the', 'be-all-and-end-all']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern2 = r\"\"\"(?x)                   # set flag to allow verbose regexps\n",
    "               (?:[A-Z]\\.)+           # don't break up abbreviations, e.g. U.S.A.\n",
    "               |\\d+(?:\\.\\d+)?%?       # allow numbers, incl. currency and percentages\n",
    "               |\\w+(?:[-']\\w+)*       # words w/ optional internal hyphens/apostrophe\n",
    "               |(?:[+/\\-@&*])         # special characters with meanings\n",
    "             \"\"\"\n",
    "text1=\"My cat weighs 5 kg, +/- 2 grams.\"\n",
    "text2=\"I live in the U.S.A.\"\n",
    "text3=\"I'm 100% sure it costs $40.00.\"\n",
    "text4=\"What's the be-all-and-end-all?\"\n",
    "print(re.findall(pattern2, text1))\n",
    "print(re.findall(pattern2, text2))\n",
    "print(re.findall(pattern2, text3))\n",
    "print(re.findall(pattern2, text4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZtM5-s-Rysd"
   },
   "source": [
    "## 5 Tokenization using Spacy\n",
    "\n",
    "5.1 Install required dependencies and download dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 18596,
     "status": "ok",
     "timestamp": 1661220652645,
     "user": {
      "displayName": "Bonnie Dorr",
      "userId": "05562801915063225516"
     },
     "user_tz": 240
    },
    "id": "cVYQcFrBRyP7",
    "outputId": "e9ecd7bf-bb7f-491f-d66e-eb4028af522a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_md\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K77a-vCuSbbD"
   },
   "source": [
    "5.2 Import dependencies and load dataset for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wDEvYLMESdmH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run this cell to import dependencies for tokenization\n",
    "import spacy\n",
    "from spacy.tokens.token import Token\n",
    "from typing import List\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "sample_text = \"My cat weighs 5 kg, +/- 2 grams.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cM1nVz0QS7CB"
   },
   "source": [
    "5.3 Run spaCy tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 215,
     "status": "ok",
     "timestamp": 1661220676101,
     "user": {
      "displayName": "Bonnie Dorr",
      "userId": "05562801915063225516"
     },
     "user_tz": 240
    },
    "id": "fcBJ0Y0STC7Q",
    "outputId": "19fdefbb-31ad-4550-f49e-5b48211d9e50",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text:  My cat weighs 5 kg, +/- 2 grams.\n",
      "Tokenizer output:  [My, cat, weighs, 5, kg, ,, +, /-, 2, grams, .]\n"
     ]
    }
   ],
   "source": [
    "#Run tokenization\n",
    "def tokenize(text: str) -> List[Token]:\n",
    "  \"\"\"\n",
    "  :param text: text as a pythong string object\n",
    "  :return: a list of objects of type Token\n",
    "  \"\"\"\n",
    "  doc = nlp(text) # spacy converts the given text into a list of tokens\n",
    "  return [w for w in doc]\n",
    "\n",
    "print(\"Sample Text: \", sample_text)\n",
    "print(\"Tokenizer output: \", tokenize(sample_text))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Module 2a - Simple-Tokenizer.ipynb",
   "provenance": [
    {
     "file_id": "1msTh82OLf_yfb8slk1Ok7gM21E_tp3Zp",
     "timestamp": 1661800779336
    },
    {
     "file_id": "1utV1POnGB5QtPlS9rwPjpr-xX3KNB0j9",
     "timestamp": 1661035956481
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
